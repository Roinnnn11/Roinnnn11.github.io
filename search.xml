<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2024/11/10/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a class="link"   href="https://hexo.io/" >Hexo <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>! This is your very first post. Check <a class="link"   href="https://hexo.io/docs/" >documentation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> for more info. If you get any problems when using Hexo, you can find the answer in <a class="link"   href="https://hexo.io/docs/troubleshooting.html" >troubleshooting <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start-你好"><a href="#Quick-Start-你好" class="headerlink" title="Quick Start 你好"></a>Quick Start 你好</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/writing.html" >Writing <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/server.html" >Server <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/generating.html" >Generating <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/one-command-deployment.html" >Deployment <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
]]></content>
  </entry>
  <entry>
    <title>PyTorch101 --PyTorch基本框架</title>
    <url>/2024/11/12/PyTorch101/</url>
    <content><![CDATA[<h3 id="一、Tensor-张量"><a href="#一、Tensor-张量" class="headerlink" title="一、Tensor 张量"></a>一、Tensor 张量</h3><p>定义：The number of dimensions is the <strong>rank</strong> of the tensor；例：tensor([1,2,3])的秩为1</p>
<p>​		the <strong>shape</strong> of a tensor is a tuple of integers giving the size of the array along each dimension 例：tensor([1,2,3])的shape为[3]</p>
<h4 id="1-构建-访问tensor"><a href="#1-构建-访问tensor" class="headerlink" title="1.构建&amp;访问tensor"></a>1.构建&amp;访问tensor</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])<span class="comment">#构建tensor张量</span></span><br><span class="line">x.dim()<span class="comment">#返回纬度值</span></span><br><span class="line">x.shape<span class="comment">#返回：torch.Size([3,3]),可以当作turple使用</span></span><br><span class="line">x[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">1</span><span class="comment">#赋值操作</span></span><br><span class="line">x[<span class="number">0</span>][<span class="number">0</span>].item()<span class="comment">#将PyTorch的标量转换为python标量</span></span><br></pre></td></tr></table></figure></div>



<h4 id="2-帮助构建tensor的函数"><a href="#2-帮助构建tensor的函数" class="headerlink" title="2.帮助构建tensor的函数"></a>2.帮助构建tensor的函数</h4><p>常用的函数有：</p>
<ul>
<li><pre><code class="python">- x=torch.zeros（*size） #全为0的tensor
- x=torch.ones(*size)#全为1的tensor
- x=torch.rand(*size)#为0-1随机数
- x=torch.full（*size*， *fill_value*， ***， *out* *=* *None*， *dtype* *=* None， *layout* *=* *torch.strided*， *device* *=* *None*， *requires_grad* *=* *False*）#→ Tensor
- x=torch.from_numpy( ndarray ) #→ Tensor
- x=torch.arange( *start=0* , *end* , *step=1* , * , out=None , *dtype=None* , layout *=torch.strided* , *device=None* , *require_grad=False* )#→Tensor &lt;u&gt;#左闭右开区间&lt;/u&gt;（torch.range &lt;u&gt;#左闭右闭&lt;/u&gt;）
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">You can find a **full list of tensor creation operations** [in the documentation](https://www.google.com/url?q=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftorch.html%23creation-ops).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 3.tensor的数据形式</span><br><span class="line"></span><br><span class="line">tensor会根据数字自动设定形式，你也可以为其赋予特定格式如：int16、uint8、float32等等。</span><br><span class="line"></span><br><span class="line">可以使用  .to( )来修改dtype，如：x0.to(torch.float32)</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">x1=torch.zeros_like(x0) #构建一个size和dtype与x0一样的</span><br><span class="line">x2=x0.new_zeros(4,5)#size自定，dtype和x0一样</span><br><span class="line">x3=torch.ones(6,7).to(x0)#和上一行相同</span><br></pre></td></tr></table></figure></div>
</code></pre>
</li>
</ul>
<p>常用的数据形式：</p>
<ul>
<li><code>torch.float32</code>: Standard floating-point type; used to store learnable parameters, network activations, etc. Nearly all arithmetic is done using this type.</li>
<li><code>torch.int64</code>: Typically used to store indices</li>
<li><code>torch.bool</code>: Stores boolean values: 0 is false and 1 is true</li>
<li><code>torch.float16</code>: Used for mixed-precision arithmetic, usually on NVIDIA GPUs with <a class="link"   href="https://www.google.com/url?q=https://www.nvidia.com/en-us/data-center/tensorcore/" >tensor cores <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>. You won’t need to worry about this datatype in this course.</li>
</ul>
<h4 id="4-Tensor-Indexing"><a href="#4-Tensor-Indexing" class="headerlink" title="4.Tensor Indexing"></a>4.Tensor Indexing</h4><p>tensor也能进行“切片”，多维度也行</p>
]]></content>
  </entry>
  <entry>
    <title>CS231N Lecture3 线性分类器</title>
    <url>/2024/11/19/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    <content><![CDATA[<h1 id="Linear-Classifiers–线性分类器"><a href="#Linear-Classifiers–线性分类器" class="headerlink" title="Linear Classifiers–线性分类器"></a>Linear Classifiers–线性分类器</h1><h3 id="一、如何理解线性分类器"><a href="#一、如何理解线性分类器" class="headerlink" title="一、如何理解线性分类器"></a>一、如何理解线性分类器</h3><h5 id="一、代数观点分析"><a href="#一、代数观点分析" class="headerlink" title="一、代数观点分析"></a>一、代数观点分析</h5><p>线性分类器：权重矩阵W和像素X之间的矩阵相乘，再加上b</p>
<p><em>如果输入数据具有native vector form，可以将b合并至W矩阵中处理**（针对线性分类是好的方法，对于卷积并非）</em></p>
<p><u>feature：预测也是线性的，放大&#x2F;缩小所有像素，会让所有预测值都放大&#x2F;缩小</u></p>
<p>（如下图）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture1/image-20241119183936618.png"
                      alt="image"
                ></p>
<h5 id="二、视觉角度分析"><a href="#二、视觉角度分析" class="headerlink" title="二、视觉角度分析"></a>二、视觉角度分析</h5><p>另一种处理方法：<u>将权重矩阵reshape到跟输入图像一样，实现<strong>“template matching”</strong></u></p>
<p>例：取每一行数字，组成2x2 shape（图像假设为2x2），同时b也分类。</p>
<p><strong>“template matching”</strong>(模式匹配)</p>
<p>按上述处理之后，每一行都是一个种类，当他们匹配上时，值会最大。有一种视觉上的模式匹配（如下图）</p>
<p>（visual viewpoint）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture1/image-20241119185407644.png"
                      alt="image"
                ></p>
<p><u>（缺点：对input图像的依赖严重，对其content（环境）依赖，旋转图片后无法识别）</u></p>
<h5 id="三、几何观点"><a href="#三、几何观点" class="headerlink" title="三、几何观点"></a>三、几何观点</h5><p>可以认为，像素图像是一个高维欧几里得空间，每个种类都有对应的一个超平面，将空间切割。（如下图）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture1/image-20241119191101704.png"
                      alt="image"
                ></p>
<h3 id="二、如何选择矩阵W-——使用损失函数来量化评估W"><a href="#二、如何选择矩阵W-——使用损失函数来量化评估W" class="headerlink" title="二、如何选择矩阵W ——使用损失函数来量化评估W"></a>二、如何选择矩阵W ——使用损失函数来量化评估W</h3><h5 id="1️⃣loss：multi-class-SVM损失（图像分类）"><a href="#1️⃣loss：multi-class-SVM损失（图像分类）" class="headerlink" title="1️⃣loss：multi-class SVM损失（图像分类）"></a>1️⃣loss：multi-class SVM损失（图像分类）</h5><p>特征：如果一个种类被正确区分，那么改变预测分数，不再影响损失。（达到零损失）</p>
<p>（如图，图中为铰链损失）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture1/IMG_0514.PNG"
                      alt="\images\IMG_0514.PNG"
                ></p>
<h5 id="2️⃣Cross-Entropy-Loss（multinomial-logistic-regression）"><a href="#2️⃣Cross-Entropy-Loss（multinomial-logistic-regression）" class="headerlink" title="2️⃣Cross- Entropy Loss（multinomial logistic regression）"></a>2️⃣Cross- Entropy Loss（multinomial logistic regression）</h5><p>want to interpret raw classifier scores as probabilities.（使用概率来评分）</p>
<p>使用softmax function 求概率 （重要工具）</p>
<p><strong>对loss的计算：L &#x3D; -logP（Y&#x3D;y_{i}&#x2F;X&#x3D;x_{i}）</strong></p>
<p>feature: 这个loss函数永远不会达到0️⃣损失</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/IMG_0522.PNG"
                      alt="\images\IMG_0522.PNG"
                ></p>
<p>对预测值和理论值进行评估<strong>（Kullback-Leibler divergence）</strong></p>
<p><strong>cross- Entropy：H（P,Q）&#x3D;H(p)+D_{KL}（P||Q）</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture1/IMG_0523.PNG"
                      alt="\images\IMG_0523.PNG"
                ></p>
<h3 id="三、对矩阵进行优化"><a href="#三、对矩阵进行优化" class="headerlink" title="三、对矩阵进行优化"></a>三、对矩阵进行优化</h3><p>Regularization（正则化）—— prevent the model from doing too well</p>
<p>lamda —— 控制正则化的超参</p>
<p>公式如图</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture1/IMG_0516.PNG"
                      alt="\images\IMG_0516.PNG"
                ></p>
<p>正则化简单实例：L2、L1、Elastic net（L1+L2）（如上图左下）</p>
<p>正则化目的：1️⃣表达偏好</p>
<p>​						2️⃣避免过拟合prefer simple models that generalize well</p>
<p>​						3️⃣添加曲率改善优化adding curvature improve optimization</p>
<p>举例说明正则化的作用：</p>
<p>1️⃣比如可以通过调整正则化、考虑全局或者专注于一个参数</p>
<p>如果有噪声或者很多特征,也适用。<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture1/IMG_0517.PNG"
                      alt="images\IMG_0517.PNG"
                ></p>
<p>2️⃣</p>
<p>避免过拟合。如图，曲线是添加正则化后，合理地减少了噪声干扰</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture1/IMG_0518.PNG"
                      alt="image"
                ></p>
]]></content>
      <categories>
        <category>CS231N</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231N Lecture4 优化</title>
    <url>/2024/12/07/%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>2️⃣ follow the slope</p>
<p>直接计算导数、使用导数（deltaloss&#x2F;delta-h）</p>
<p>缺点：对求导计算缓慢，如果矩阵&#x2F;维度过大</p>
<p>&#x3D;&#x3D;&#x3D;用一个方程表示梯度（反向传播 第六节中讲述）</p>
<p><strong>Computing Gradients(计算梯度)</strong></p>
<p>numeric&#x2F;analytic （计算出值&#x2F;分析）</p>
<p>一般使用数字梯度来检验对解析梯度的推导</p>
<p>（pytorch提供了一个函数gradcheck、实现相似的功能）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0531.PNG"
                      alt="image"
                ></p>
<h3 id="gradient-descent梯度下降"><a href="#gradient-descent梯度下降" class="headerlink" title="gradient descent梯度下降"></a><u>gradient descent梯度下降</u></h3><p>包含的超参：</p>
<p><u>1️⃣初始化权重的方法</u></p>
<p><u>2️⃣循环次数：受计算预算和时间限制</u></p>
<p><u>3️⃣学习率learning rate：</u></p>
<p>较大：收敛较快</p>
<p>较小：不容易数字爆炸、但时间较长</p>
<p>梯度下降的feature：使用全部数据，当数据集较大时，不适用</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0532.PNG"
                      alt="image"
                ></p>
<h3 id="batch-gradient-descent（批量梯度下降）"><a href="#batch-gradient-descent（批量梯度下降）" class="headerlink" title="batch gradient descent（批量梯度下降）"></a><u>batch gradient descent（批量梯度下降）</u></h3><p>选择批量，进行求loss和计算梯度</p>
<p>SGD随机梯度下降</p>
<h3 id="stochastic-gradient-descentSGD随机梯度下降"><a href="#stochastic-gradient-descentSGD随机梯度下降" class="headerlink" title="stochastic gradient descentSGD随机梯度下降"></a><u>stochastic gradient descentSGD随机梯度下降</u></h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0533.PNG"
                      alt="image"
                ></p>
<p>存在的问题：</p>
<p><u>局部最小、鞍点&#x2F;收敛速度一方过大一方过小</u></p>
<p><strong>更智能的方法：</strong></p>
<h4 id="1-sgd-动量更新"><a href="#1-sgd-动量更新" class="headerlink" title="1.sgd+动量更新"></a>1.sgd+动量更新</h4><p>初始化相同、用梯度算出”dw”的值，用来更新速度向量，（想象球滚落）（类似于对梯度计算历史平均值）</p>
<p>超参数：<strong>rho– decay rate</strong></p>
<p><u>（当抵达局部最小&#x2F;鞍点，仍有残余速度去跳出局部，更平滑地处理高纬度问题）</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0555.PNG"
                      alt="image"
                ></p>
<h4 id="2-Nesterov-动量更新"><a href="#2-Nesterov-动量更新" class="headerlink" title="2.Nesterov 动量更新"></a>2.Nesterov 动量更新</h4><p>如图</p>
<p>“Look ahead”会对下一时刻的梯度进行预测</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0559.PNG"
                      alt="image"
                ></p>
<p>两者对比：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0557.PNG"
                      alt="image"
                ></p>
<p><u>sgd特征：1.可能会overshoot（到达底端后，会有反向动量存在，往回走一段距离，如下图）</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0560.PNG"
                      alt="image"
                ></p>
<h3 id="Adaptive-learning-rates自适应学习率"><a href="#Adaptive-learning-rates自适应学习率" class="headerlink" title="Adaptive learning rates自适应学习率"></a>Adaptive learning rates自适应学习率</h3><h4 id="1-AdaGrad算法"><a href="#1-AdaGrad算法" class="headerlink" title="1.AdaGrad算法"></a>1.AdaGrad算法</h4><p>（通过<strong>除以梯度</strong>，抑制&#x2F;提高运动速度）</p>
<p><u>问题：grad—squared不断增加（趋于无限大），学习率不断下降，可能在达到结果前停止</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0561.PNG"
                      alt="image"
                ></p>
<h4 id="2-RMSprop（对AdaGrad的改善）"><a href="#2-RMSprop（对AdaGrad的改善）" class="headerlink" title="2.RMSprop（对AdaGrad的改善）"></a>2.RMSprop（对AdaGrad的改善）</h4><p>添加参数decay_rate 来降低grad_squared,  类似加入摩擦系数</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0562.PNG"
                      alt="image"
                ></p>
<h3 id="Adam（RMSProp-动量）"><a href="#Adam（RMSProp-动量）" class="headerlink" title="Adam（RMSProp+动量）"></a>Adam（RMSProp+动量）</h3><p>——非常强大的优化算法，适用于不同的任务，超参的调整范围也较小</p>
<p>将两种算法结合，存在问题：如果beta2趋于1，那么优化第一步可能走出很大一步（除以近似0的数）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0563.PNG"
                      alt="image"
                ></p>
<p>对其进行改善：添加<strong>Bias Corection（偏差校正）</strong>，让其更稳健。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0564.PNG"
                      alt="image"
                ></p>
<h3 id="AdamW：Adam-Variant-with-Weight-Decay"><a href="#AdamW：Adam-Variant-with-Weight-Decay" class="headerlink" title="AdamW：Adam Variant with Weight Decay"></a>AdamW：Adam Variant with Weight Decay</h3><p>Source：<a class="link"   href="https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html" >https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p>在每次epoch中，对权重进行衰减：</p>
<div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">moving_avg = alpha * moving_avg + (<span class="number">1</span>-alpha) * w.grad </span><br><span class="line"></span><br><span class="line">w = w - lr * moving_avg - lr * wd * w</span><br></pre></td></tr></table></figure></div>

<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/image.png"
                      alt="image"
                ></p>
<h3 id="Learning-Rate-Decay-减少学习率"><a href="#Learning-Rate-Decay-减少学习率" class="headerlink" title="Learning Rate Decay 减少学习率"></a>Learning Rate Decay 减少学习率</h3><p>在某些固定点减少学习率。常见的方法如下：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/image0568.png"
                      alt="image"
                ></p>
<h3 id="二阶优化方法：L-BFGS（准确但不实际）-BGFS-最常用"><a href="#二阶优化方法：L-BFGS（准确但不实际）-BGFS-最常用" class="headerlink" title="二阶优化方法：L-BFGS（准确但不实际）&#x2F;BGFS(最常用)"></a>二阶优化方法：L-BFGS（准确但不实际）&#x2F;BGFS(最常用)</h3><p>使用Hessian矩阵，可以使对学习率&#x2F;步长的选择更加合理。但数据量巨大，只适合低维优化问题</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0567.PNG"
                      alt="image"
                ></p>
<h3 id="总结（对优化算法的比较）"><a href="#总结（对优化算法的比较）" class="headerlink" title="总结（对优化算法的比较）"></a>总结（对优化算法的比较）</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture2/IMG_0566.PNG"
                      alt="image"
                ></p>
]]></content>
      <categories>
        <category>CS231N</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231N Lecture8 CNN经典架构</title>
    <url>/2025/01/17/8%20CNN%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h1 id="Lecture8-CNN经典架构"><a href="#Lecture8-CNN经典架构" class="headerlink" title="Lecture8 CNN经典架构"></a>Lecture8 CNN经典架构</h1><h3 id="Alexnet"><a href="#Alexnet" class="headerlink" title="Alexnet"></a>Alexnet</h3><p>input：227x227x3</p>
<p>① CONV1: 96 11x11 filters at stride 4 pad 2.</p>
<p>输出？W’ &#x3D; (W-k+2p)&#x2F;s +1&#x3D;55 [55x55x96]</p>
<p>总参数？(11<em>11</em>3+1)<em>96 &#x3D; 35k 一层有：（输入通道</em>内核大小+偏差）</p>
<p>浮点运算（乘法和加法）? （C_out<em>H’<em>W’）</em>(C_in</em>k*k)</p>
<p>② POOL1:  3x3 filters at stride 2 pad 1</p>
<p>输出？[27x27x96]向下取整</p>
<p>浮点运算？(C_out<em>H’<em>W’)</em>(K</em>K)</p>
<p>不改变通道数量</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image1.png"
                      alt="image"
                ></p>
<p>总结规律：1.<u>池化的计算次数远小于卷积</u><br><u>2.主要的内存使用在早期卷积层</u><br><u>3.参数主要在全连接层中</u><br><u>4.主要的浮点运算在卷积层</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image2.png"
                      alt="image"
                ></p>
<h3 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h3><p><strong>设计规则</strong>：</p>
<ol>
<li><ol>
<li><ol>
<li><strong>All conv 3x3 stride 1 pad 1</strong></li>
<li><strong>All max pool 2x2 stride 2</strong></li>
<li><strong>After pool, double channels.</strong></li>
</ol>
</li>
<li></li>
</ol>
</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image3.png"
                      alt="image"
                ></p>
<p>为什么使用小卷积层？<u>三个3x3层与一个7x7层的概念域相等，计算更少，允许更多非线性计算。</u></p>
<p>为什么双倍通道？<u>这样做之后，该层与上一层计算次数相同。</u></p>
<h3 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h3><h4 id="1-Aggressive-Stem"><a href="#1-Aggressive-Stem" class="headerlink" title="1.Aggressive Stem"></a>1.Aggressive Stem</h4><p>用stem network在开始时采样，减小空间开销</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image4.png"
                      alt="image"
                ></p>
<h4 id="2-Inception-Module-初始模块（主要组成）"><a href="#2-Inception-Module-初始模块（主要组成）" class="headerlink" title="2.Inception Module 初始模块（主要组成）"></a>2.Inception Module 初始模块（主要组成）</h4><p>使用并行处理，在同一时间进行多个卷积</p>
<p>在卷积前使用池化，减少通道数量。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image5.png"
                      alt="image"
                ></p>
<h4 id="3-全局平均池化"><a href="#3-全局平均池化" class="headerlink" title="3.全局平均池化"></a>3.全局平均池化</h4><p>用全局平均池化取代全连接层，对通道求平均，减少元素总数，来摧毁空间。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image6.png"
                      alt="image"
                ></p>
<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><h4 id="1-Residual-Block"><a href="#1-Residual-Block" class="headerlink" title="1.Residual Block"></a>1.Residual Block</h4><p>使深网络更好地模拟浅层网络，改善梯度流。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image7.png"
                      alt="image"
                ></p>
<h4 id="“Bottleneck-Block”："><a href="#“Bottleneck-Block”：" class="headerlink" title="“Bottleneck Block”："></a>“Bottleneck Block”：</h4><p>1x1收缩通道-3x3卷积-1x1扩张通道。</p>
<p>增加了层数，但计算复杂度不变，减少误差。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image8.png"
                      alt="image"
                ></p>
<h4 id="2-使用了GoogleNet的向下采样以及全局平均池化的方法。"><a href="#2-使用了GoogleNet的向下采样以及全局平均池化的方法。" class="headerlink" title="2.使用了GoogleNet的向下采样以及全局平均池化的方法。"></a>2.使用了GoogleNet的<strong>向下采样</strong>以及<strong>全局平均池化</strong>的方法。</h4><h3 id="ResNeXt"><a href="#ResNeXt" class="headerlink" title="ResNeXt"></a>ResNeXt</h3><p>使用<strong>并行路径</strong>，计算成本与左侧相同。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture8/image9.png"
                      alt="image"
                ></p>
]]></content>
      <categories>
        <category>CS231N</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231N Lecture5 神经网络</title>
    <url>/2024/12/08/5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>“线性分类器不够强大”</p>
<p>Solution ①：</p>
<h5 id="Feature-Transform-“特征变换”"><a href="#Feature-Transform-“特征变换”" class="headerlink" title="Feature Transform “特征变换”"></a>Feature Transform “特征变换”</h5><p>————需要考虑如何设计特征类型</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image1.png"
                      alt="image"
                ></p>
<h6 id="举例一：颜色直方图-Color-Histogram"><a href="#举例一：颜色直方图-Color-Histogram" class="headerlink" title="举例一：颜色直方图 Color Histogram"></a>举例一：颜色直方图 Color Histogram</h6><p>只考虑颜色出现的频率&#x2F;多少，不考虑图像实际信息</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image2.png"
                      alt="image"
                ></p>
<h6 id="举例二：Histogram-of-Oriented-Gradients（HoG）"><a href="#举例二：Histogram-of-Oriented-Gradients（HoG）" class="headerlink" title="举例二：Histogram of Oriented Gradients（HoG）"></a>举例二：Histogram of Oriented Gradients（HoG）</h6><p>主要流程：</p>
<p>1.对每个像素边缘方向、强度计算。</p>
<p>2.将图像分为8x8 区域</p>
<p>3.在每个区域，计算HoG。</p>
<p>常用于物体检测等任务（20世纪初）。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image3.png"
                      alt="image"
                ></p>
<h6 id="举例三：”data-driven-数据驱动型”"><a href="#举例三：”data-driven-数据驱动型”" class="headerlink" title="举例三：”data driven 数据驱动型”"></a>举例三：”data driven 数据驱动型”</h6><p>将数据集图片内容提取，使用聚类，得到与视觉词对应的”codebook”（形成特征向量）</p>
<p>再对图片进行处理，绘制图片的颜色直方图，将其与codebook匹配，寻找结果。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image4.png"
                      alt="image"
                ></p>
<p><u>缺点：特征提取复杂；不能有效利用数据自行调整系统、达到最好的分类效果</u></p>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>如图，是一个<strong>简单的神经网络</strong>示例：由<u>输入列向量x，权重矩阵W1、W2，隐藏层h，输出层S</u>构成。</p>
<p>其中的权重矩阵每个元素都有值，对后续输出造成影响，称为<strong>全连接神经网络&#x2F;多层感知器</strong>  “Fully-connected neural network”&#x2F;“Multi-Layer Perceptron”（MLP）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image5.png"
                      alt="image"
                ></p>
<p>复杂神经网络</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image6.png"
                      alt="image"
                ></p>
<h4 id="常用Activation-Function-–RELU函数"><a href="#常用Activation-Function-–RELU函数" class="headerlink" title="常用Activation Function –RELU函数"></a>常用Activation Function –RELU函数</h4><p>RELU函数（修正线性单元）–使用最广泛的激活函数</p>
<p><del>如果不添加激活函数，神经网络又是一个线性分类器！</del></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image7.png"
                      alt="image"
                ></p>
<h4 id="其他常用激活函数："><a href="#其他常用激活函数：" class="headerlink" title="其他常用激活函数："></a>其他常用激活函数：</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image8.png"
                      alt="image"
                ></p>
<h5 id="从一个有趣的角度理解激活函数：空间扭曲"><a href="#从一个有趣的角度理解激活函数：空间扭曲" class="headerlink" title="从一个有趣的角度理解激活函数：空间扭曲"></a>从一个有趣的角度理解激活函数：空间扭曲</h5><p>线性分类器造成的空间扭曲，大概率是这样的：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image9.png"
                      alt="image"
                ></p>
<p>但是使用RELU函数，对空间变换如下：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image10.png"
                      alt="image"
                ></p>
<p>这样的话，数据在变换后的特征空间变得线性可分（如下图，类似于对空间进行两次折叠）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image11.png"
                      alt="image"
                ></p>
<h4 id="增加隐藏层，可能导致决策边界变复杂"><a href="#增加隐藏层，可能导致决策边界变复杂" class="headerlink" title="增加隐藏层，可能导致决策边界变复杂"></a>增加隐藏层，可能导致决策边界变复杂</h4><p>——需采用<strong>更强的正则化</strong>，<del>而不是减少隐藏层！</del></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image12.png"
                      alt="image"
                ></p>
<h4 id="神经网络另一功能：Universal-Approximation-“万能逼近”"><a href="#神经网络另一功能：Universal-Approximation-“万能逼近”" class="headerlink" title="神经网络另一功能：Universal Approximation “万能逼近”"></a>神经网络另一功能：Universal Approximation “万能逼近”</h4><p>从代数观点分析：</p>
<p>对于结果y，每一项u1<em>max(0,w1</em>x+b1)相当于RELU函数的平移&#x2F;放大缩小：</p>
<p>根据wi正负对RELU进行左右翻转；</p>
<p>根据偏差bi来设置拐点；</p>
<p>根据第二个权重&#x2F;第一个权重得到斜率。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image13.png"
                      alt="image"
                ></p>
<p>例：用四个隐藏层（四个relu函数），完成对凹凸函数的拟合</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image14.png"
                      alt="image"
                ></p>
<h4 id="关于神经网络的优化–"><a href="#关于神经网络的优化–" class="headerlink" title="关于神经网络的优化–"></a>关于神经网络的优化–</h4><p>1.<strong>NONConvex Functions</strong> “非凸函数优化”</p>
<p>目前没有理论验证其一定收敛，但实践证明有用。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture3/image15.png"
                      alt="image"
                ></p>
]]></content>
      <categories>
        <category>CS231N</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231N Lecture6 反向传播</title>
    <url>/2024/12/10/6%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<h1 id="Lecture6–反向传播"><a href="#Lecture6–反向传播" class="headerlink" title="Lecture6–反向传播"></a>Lecture6–反向传播</h1><p><em>如何为神经网络计算梯度？</em></p>
<p>bad idea：在纸上推导</p>
<p>better：计算图</p>
<p>以svm  loss举例</p>
<p>蓝色节点：x与W的矩阵乘法</p>
<p>红色节点：铰链损失（针对SVMloss）</p>
<p>绿色：正则化项</p>
<p>相加得到L（loss）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image1.png"
                      alt="image"
                ></p>
<h2 id="Backpropagation-反向传播"><a href="#Backpropagation-反向传播" class="headerlink" title="Backpropagation 反向传播"></a>Backpropagation 反向传播</h2><p>构成：</p>
<p>1<u>.Forward pass</u> “前向传递”计算输出值</p>
<p>2.<u>backward pass</u> “反向传递”计算每个参数的导数</p>
<p><u>下游梯度 &#x3D; 局部梯度 x 上游梯度</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image2.png"
                      alt="image"
                ></p>
<h5 id="优点："><a href="#优点：" class="headerlink" title="优点："></a><u>优点：</u></h5><p><u>将对梯度的计算<strong>模块化</strong>。不需要知道全局架构，只需要知道这个节点里对应的三个梯度数值（上游&#x2F;本地&#x2F;下游），从而推出<strong>全局梯度</strong>。</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image3.png"
                      alt="image"
                ></p>
<h5 id="全局图be-like："><a href="#全局图be-like：" class="headerlink" title="全局图be like："></a>全局图be like：</h5><p>（trick: 蓝色框内为sigmoid函数，可以直接计算其local gradient得到简单表达式,跳过中间步骤)</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image4.png"
                      alt="image"
                ></p>
<h3 id="梯度流动时一些有趣的pattern："><a href="#梯度流动时一些有趣的pattern：" class="headerlink" title="梯度流动时一些有趣的pattern："></a>梯度流动时一些有趣的pattern：</h3><p>加法：downstream gradient &#x3D; upstream gradient</p>
<p>复制：downstream gradient &#x3D; sum(upstream gradient)</p>
<p>乘法：“交换”downstream gradient &#x3D; other diwnstream gradient * upstream</p>
<p>max：最大值downstream &#x3D; upstream，其余downstream&#x3D;0（不常见）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image5.png"
                      alt="image"
                ></p>
<p>实际处理问题时，我们通常是对<strong>向量</strong>进行求梯度等操作（最后得到的loss仍然是标量）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image6.png"
                      alt="image"
                ></p>
<p>由于只考虑upstream和downstream的关系，Jocobian矩阵将会是一个<em>非常大的稀疏矩阵</em>，<em>只有对角线上元素可能不为0</em>.所以在使用中，从来不会真正形成矩阵，而是对其<strong>隐式表达</strong>。</p>
<p>如下图，对RELU函数，可以理解为：</p>
<p>根据input符号，决定downstream是0还是具体值</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image7.png"
                      alt="image"
                ></p>
<p>当使用的是<strong>tensor</strong>，更复杂了.</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image8.png"
                      alt="image"
                ></p>
<h3 id="简化求解反向传播"><a href="#简化求解反向传播" class="headerlink" title="简化求解反向传播"></a>简化求解反向传播</h3><p>推导：分解问题，尝试对每个x的导数求解</p>
<p>如图，dL&#x2F;dx1,1 &#x3D; (dy&#x2F;dx1,1)(dL&#x2F;dy)</p>
<p>计算dy&#x2F;dx1,1，发现其为第一行等于权重矩阵第一行，其余行为0的矩阵；</p>
<p>所以，dL&#x2F;dx1,1等于权重第一行与dL&#x2F;dy第一行的内积；</p>
<p>其他同理。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image9.png"
                      alt="image"
                ></p>
<p>最后推得关系式如下：dL&#x2F;dx  &#x3D; (dL&#x2F;dy)wT</p>
<p>（详细证明：<a class="link"   href="http://cs231n.stanford.edu/handouts/linear-backprop.pdf%EF%BC%89" >http://cs231n.stanford.edu/handouts/linear-backprop.pdf） <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image10.png"
                      alt="image"
                ></p>
<h3 id="另一个观点：反向自动微分"><a href="#另一个观点：反向自动微分" class="headerlink" title="另一个观点：反向自动微分"></a>另一个观点：反向自动微分</h3><h5 id="反向自动微分"><a href="#反向自动微分" class="headerlink" title="反向自动微分"></a>反向自动微分</h5><p>认为jacobian矩阵可以累乘，最后得到一个标量。（对于所有的与求导&#x2F;微分相关的程序都有效，局限性小）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image11.png"
                      alt="image"
                ></p>
<h5 id="前向自动微分"><a href="#前向自动微分" class="headerlink" title="前向自动微分"></a>前向自动微分</h5><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image12.png"
                      alt="image"
                ></p>
<p><u>缺点：前向不被大框架支持；不好用</u></p>
<h3 id="反向传播另一个功能：求高阶导数"><a href="#反向传播另一个功能：求高阶导数" class="headerlink" title="反向传播另一个功能：求高阶导数"></a>反向传播另一个功能：求高阶导数</h3><p>（比如计算hessian矩阵）</p>
<p>使用反向传播扩展计算图。在计算loss之后，使用f2’计算梯度相对于x1的损失，用f1’计算loss相对于x0的损失（f1’\f2’是f的反向传递）</p>
<p>再点积向量 v，就会得到v关于x的导数。</p>
<p>再Backprop，可以得到x关于v的导数。</p>
<p>（图中举例：二阶导数）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture6/image13.png"
                      alt="image"
                ></p>
]]></content>
      <categories>
        <category>CS231N</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231N-Lecture7 卷积神经网络</title>
    <url>/2025/01/15/7%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="Lecture7-卷积神经网络"><a href="#Lecture7-卷积神经网络" class="headerlink" title="Lecture7 卷积神经网络"></a>Lecture7 卷积神经网络</h1><p>前面存在的问题：并没有利用图像的空间结构（将其展开成向量）</p>
<h2 id="1-Convolution-layer-卷积层"><a href="#1-Convolution-layer-卷积层" class="headerlink" title="1.Convolution layer 卷积层"></a>1.Convolution layer 卷积层</h2><p>超参数：<strong>（卷积层大小、层数、填充、步长）</strong></p>
<p>构成：<strong>①输入三维张量</strong>（depth x width x height）</p>
<p><strong>②权重矩阵 filter</strong>（也是三维）</p>
<p>存在<strong>约束</strong>：filter与input的depth必须相同。（filter会覆盖input的整个深度）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image1.png"
                      alt="image"
                ></p>
<p>计算：</p>
<p>将filter在输入张量上滑动，选定一块区域进行点积，再加上偏差，得到一个标量结果。对input所有可能的位置进行该操作。</p>
<p>使用举例：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image2.png"
                      alt="image"
                ></p>
<h3 id="填充（padding）"><a href="#填充（padding）" class="headerlink" title="填充（padding）"></a><strong>填充（padding）</strong></h3><p><u>卷积时会造成像素损失，所以在图片周围进行填充，以减少损失（如下：0填充）</u></p>
<p><strong><u>same padding</u></strong>(不改变空间大小)：将p设置为（k-1）&#x2F;2</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image3.png"
                      alt="image"
                ></p>
<h3 id="步长Stride"><a href="#步长Stride" class="headerlink" title="步长Stride"></a><strong><u>步长Stride</u></strong></h3><p>定义：<u>一次移动一个步长，会将概念域翻对应倍数。</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image4.png"
                      alt="image"
                ></p>
<h3 id="概念域Receptive-Field"><a href="#概念域Receptive-Field" class="headerlink" title="概念域Receptive Field"></a>概念域Receptive Field</h3><p>定义：<u>输出张量的一个元素与input的局部区域对应，所对应的域即为概念域。</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image5.png"
                      alt="image"
                ></p>
<p>举例：<strong>1x1 卷积</strong></p>
<p>对空间中每一个网格的特征向量操作，用来改变三维张量的通道维数。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image6.png"
                      alt="image"
                ></p>
<p>举例：<strong>全连接层 （fully connected layer）</strong></p>
<p>用来展开张量、破坏空间结构，得到一个向量输出。</p>
<p>总结：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture7/image7.png"
                      alt="image"
                ></p>
<p>其他卷积：<strong>一维卷积</strong>：（例如处理音频数据）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image8.png"
                      alt="image"
                ></p>
<p><strong>三维卷积</strong>：（例如处理点云数据）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image9.png"
                      alt="image"
                ></p>
<h3 id="2-池化层-Pooling-layer：向下采样"><a href="#2-池化层-Pooling-layer：向下采样" class="headerlink" title="2.池化层 Pooling layer：向下采样"></a>2.池化层 Pooling layer：向下采样</h3><p>包含de超参：<strong>内核大小、步长、池化函数</strong></p>
<p>举例：<strong>最大池化max pooling</strong></p>
<p><u>空间维度减半</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image10.png"
                      alt="image"
                ></p>
<h3 id="3-正则化"><a href="#3-正则化" class="headerlink" title="3.正则化"></a>3.正则化</h3><p>形成零均值和零平均单位方差，对数据处理，</p>
<p>作用：<u>1.稳定加速神经网络的训练。</u></p>
<p><u>改善梯度流动。</u></p>
<p><u>允许更高的学习率，更快的收敛。</u></p>
<p><u>更加鲁棒性</u>。</p>
<p>一般<strong>放在全连接层后，非线性函数前</strong></p>
<p>举例：①<strong>批正则化</strong>（xk-均值&#x2F;标准差）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image11.png"
                      alt="image"
                ></p>
<p>N：<strong>批量参数</strong>（n个向量）</p>
<p>D：<strong>向量维数</strong></p>
<p>添加<strong>学习尺度、偏差bias</strong>两个参数，生成新的输出</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image12.png"
                      alt="image"
                ></p>
<p>测试时：<u>所用的\mu和\thgma都是对训练中的值求平均（两个常数），归一化变成线性操作。</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image13.png"
                      alt="image"
                ></p>
<p>存在问题：1.优化原理不清晰 2.训练&#x2F;测试时方法不同</p>
<p>举例：<strong>②层标准化Layer Normalization</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image14.png"
                      alt="image"
                ></p>
<p>举例：③<strong>实例归一化Instance Normalization</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image15.png"
                      alt="image"
                ></p>
<p><strong>三者区别:</strong></p>
<p>批正则：对批和空间正则</p>
<p>层：对空间和通道正则</p>
<p>实例：对通道正则</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture7/image16.png"
                      alt="image"
                ></p>
]]></content>
      <categories>
        <category>CS231N</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231N Lecture10&amp;11 训练技巧</title>
    <url>/2025/01/23/CS231N-10&amp;11/</url>
    <content><![CDATA[<h1 id="1-One-time-setup"><a href="#1-One-time-setup" class="headerlink" title="1.One time setup"></a>1.One time setup</h1><h2 id="1-Activation-Function激活函数及其特征"><a href="#1-Activation-Function激活函数及其特征" class="headerlink" title="1.Activation Function激活函数及其特征"></a>1.Activation Function激活函数及其特征</h2><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>类似于“神经放电”，将数字压缩至[0,1]</p>
<p>三个问题：①平滑的地方梯度消失，让学习过程更困难  （最严重的问题）</p>
<p>​					②其输出一直为正，并不是以零中心对称的。存在非常不稳定的因素。（会导致梯度一直为正&#x2F;负）–使用小批量梯度，对这些梯度求平均得到最后的梯度，可以解决这个问题</p>
<p>​					③exp（）函数计算成本较高</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image1.png"
                      alt="image"
                ></p>
<h3 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h3><p>是sigmoid的变形。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image2.png"
                      alt="image"
                ></p>
<h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><ul>
<li>计算效率高</li>
<li>比sigmoid&#x2F;tanh收缩更快</li>
<li>问题：dead ReLU：当输入为负数，梯度将恒为零；所有输入为负，将无法激活</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image3.png"
                      alt="image"
                ></p>
<h3 id="Leaky-ReLU函数"><a href="#Leaky-ReLU函数" class="headerlink" title="Leaky ReLU函数"></a>Leaky ReLU函数</h3><p>这样梯度不会“死去”</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image4.png"
                      alt="image"
                ></p>
<h3 id="GELU函数（高斯错误线性单元）"><a href="#GELU函数（高斯错误线性单元）" class="headerlink" title="GELU函数（高斯错误线性单元）"></a>GELU函数（高斯错误线性单元）</h3><ul>
<li>在0左右取值表现好</li>
<li>其流畅度有利于训练</li>
<li>不是单调的    –可能破坏信息</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image5.png"
                      alt="image"
                ></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image6.png"
                      alt="image"
                ></p>
<h2 id="2-Data-Processing-数据预处理与传递"><a href="#2-Data-Processing-数据预处理与传递" class="headerlink" title="2.Data Processing 数据预处理与传递"></a>2.Data Processing 数据预处理与传递</h2><h3 id="常见操作"><a href="#常见操作" class="headerlink" title="常见操作"></a>常见操作</h3><ol>
<li>zero-centered 零中心</li>
<li>normalization 正则化</li>
<li>decorrelation 去相关：使用协方差矩阵旋转数据云，使数据不相关</li>
<li>whitened data 白化：同时使用去相关和正则化</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image7.png"
                      alt="image"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image8.png"
                      alt="image"
                ></p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>常见的操作：</p>
<ul>
<li>计算图像的mean值， 可以在样本中减去该平均图像</li>
<li>求平均通道，并在每个像素中减去</li>
<li>求通道的平均和三个颜色标准差，减去均值除以标准差。</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image9.png"
                      alt="image"
                ></p>
<h2 id="3-Weight-Initialization-权重初始化"><a href="#3-Weight-Initialization-权重初始化" class="headerlink" title="3.Weight Initialization 权重初始化"></a>3.Weight Initialization 权重初始化</h2><ol>
<li>小的随机数 —只对小网络有效（最简单的）</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image10.png"
                      alt="image"
                ></p>
<p><u>Activation Statistics 统计数据激活</u></p>
<h3 id="Xavier-Initialization"><a href="#Xavier-Initialization" class="headerlink" title="Xavier Initialization"></a><strong>Xavier Initialization</strong></h3><p>（思想：让输入与输出的方差相等）</p>
<p>具体实现如下：</p>
<p>（使用tanh函数、如果使用ReLU又会坍缩）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image11.png"
                      alt="image"
                ></p>
<h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p>如下</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image12.png"
                      alt="image"
                ></p>
<h3 id="Kaiming-MSRA-Initialization"><a href="#Kaiming-MSRA-Initialization" class="headerlink" title="Kaiming&#x2F;MSRA Initialization"></a>Kaiming&#x2F;MSRA Initialization</h3><p>方差为2&#x2F;Din</p>
<p>对残差网络没什么用</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image13.png"
                      alt="image"
                ></p>
<h3 id="残差网络的初始：（存疑，2024的ppt没有提及）"><a href="#残差网络的初始：（存疑，2024的ppt没有提及）" class="headerlink" title="残差网络的初始：（存疑，2024的ppt没有提及）"></a>残差网络的初始：（<u>存疑，2024的ppt没有提及</u>）</h3><p>第一层用MSRA初始化，最后一层为零。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image14.png"
                      alt="image"
                ></p>
<h2 id="3-Regularization-正则化策略"><a href="#3-Regularization-正则化策略" class="headerlink" title="3.Regularization 正则化策略"></a>3.Regularization 正则化策略</h2><h3 id="L2-regularization"><a href="#L2-regularization" class="headerlink" title="L2 regularization"></a>L2 regularization</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image15.png"
                      alt="image"
                > </p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><strong>Dropout</strong></h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image16.png"
                      alt="image"
                ></p>
<p>在层内随机使神经元变为0，<strong>随机的概率</strong>为超参数。</p>
<p>原理：<u>强制网络存在冗余表示；防止共同适应的特征出现；</u></p>
<p>或者：认为Dropout是<u>对一个共享权重的子集训练，所有子集权重相同</u>。</p>
<p>在<strong>测试</strong>时：<u>不drop（使用所有神经元），然后乘以概率，（即得到训练时对应的期望），没有随机性</u>。如下图。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image17.png"
                      alt="image"
                ></p>
<h3 id="A-common-pattern：Batch-Normalization"><a href="#A-common-pattern：Batch-Normalization" class="headerlink" title="A common pattern：Batch Normalization"></a>A common pattern：Batch Normalization</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image18.png"
                      alt="image"
                ></p>
<h2 id="4-Data-Augmentation-数据增强"><a href="#4-Data-Augmentation-数据增强" class="headerlink" title="4.Data Augmentation 数据增强"></a>4.Data Augmentation 数据增强</h2><p>可以扩大训练集，对训练增加随机性。</p>
<ul>
<li>Random Crops and Scales 对图片随机裁剪</li>
<li>Horizontal Flips 视角翻转</li>
<li>Color Jitter 随机化对比度和亮度，减少对特定颜色通道的依赖</li>
<li>Cutout 随机遮挡图像的部分区域，避免过度依赖局部特征，减少过拟合</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image19.png"
                      alt="image"
                ></p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image20.png"
                      alt="image"
                ></p>
<h1 id="2-Training-dynamics"><a href="#2-Training-dynamics" class="headerlink" title="2.Training dynamics"></a>2.Training dynamics</h1><h2 id="5-Learning-Rate-Schedules-学习率（超参）"><a href="#5-Learning-Rate-Schedules-学习率（超参）" class="headerlink" title="5.Learning Rate Schedules 学习率（超参）"></a>5.Learning Rate Schedules 学习率（超参）</h2><h3 id="①step-schedule"><a href="#①step-schedule" class="headerlink" title="①step schedule"></a>①step schedule</h3><p>从高学习率开始，在选定的点降低学习率。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image21.png"
                      alt="image"
                ></p>
<h3 id="②cosine-schedule"><a href="#②cosine-schedule" class="headerlink" title="②cosine schedule"></a>②cosine schedule</h3><p>只有两个超参数：学习率\alpha和周期T；超参数比step schedule少得多。</p>
<p>半波余弦函数。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image22.png"
                      alt="image"
                ></p>
<h3 id="③Linear-schedule"><a href="#③Linear-schedule" class="headerlink" title="③Linear schedule"></a>③Linear schedule</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image23.png"
                      alt="image"
                ></p>
<h3 id="④Inverse-Sqrt-schedule"><a href="#④Inverse-Sqrt-schedule" class="headerlink" title="④Inverse Sqrt schedule"></a>④Inverse Sqrt schedule</h3><p>学习率下降很快，更多时间在低学习率上。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image24.png"
                      alt="image"
                ></p>
<h3 id="⑤constant-schedule"><a href="#⑤constant-schedule" class="headerlink" title="⑤constant schedule"></a>⑤constant schedule</h3><p>使用常数可以让你的模型开始工作。调整学习率是长期工作（提高效率）。</p>
<p>一般在每5or10 个epoch，形成模型检查点，如果梯度爆炸（正确率降低、停止训练）</p>
<h2 id="6-超参数的选择"><a href="#6-超参数的选择" class="headerlink" title="6.超参数的选择"></a>6.超参数的选择</h2><h3 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h3><p>step 1：检查初始损失。（关闭权重衰减）根据损失函数的结构，分析计算随机初始得到的损失。—-（如果与预期不符，则存在bug）</p>
<p>step 2：过拟合小样本。在小样本（5-10个mini batch）上达到100% accuracy。在过程中调整参数如架构（architecture）、学习率、初始权重。（关闭正则化）—-（目的：确保优化过程无误）</p>
<p>step3 ：找到让loss迅速下降的学习率。使用上一步的架构、全部的数据、小的权重衰减，在一百次左右迭代中显著下降。</p>
<p>好的尝试选择：1e-1,1e-2,1e-3,1e-4</p>
<p>step4 ：建立粗略的超参数网格。通过粗略网格搜索和短期训练（1-5个epoch），快速缩小一个合适的学习率范围，为后续的精细调优（refining grid）做好准备。这一步的目的是确定一个大致的学习率区域</p>
<p>step5 ：Refeine grid。在之前粗略选出的学习率范围内，选择更多的学习率值并进行更精细的搜索。延长训练时间到10-20个epoch或更多。</p>
<p>step6 ：观察loss曲线以及accuracy曲线。分析如下。</p>
<p>step7 ：goto step5.</p>
<h3 id="（分析loss曲线）"><a href="#（分析loss曲线）" class="headerlink" title="（分析loss曲线）"></a>（分析loss曲线）</h3><ol>
<li>先平滑再下降：–可能需要重新初始化。</li>
<li>先下降再平滑： –可能是学习率太高，尝试LR decay</li>
<li>下降时step decay，然后平滑： –LR decay过早</li>
</ol>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image25.png"
                      alt="image"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image26.png"
                      alt="image"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image27.png"
                      alt="image"
                ></p>
<h3 id="（分析Accuracy曲线）"><a href="#（分析Accuracy曲线）" class="headerlink" title="（分析Accuracy曲线）"></a>（分析Accuracy曲线）</h3><ul>
<li>train&amp;val准确率仍上升： 需要更长时间训练</li>
<li>train上升，val下降，间隙大且越来越大，发生过拟合： 增强正则化&#x2F;使用更多训练数据&#x2F;（少数情况，减少模型大小或容量）</li>
<li>train与val间隙很小，没有正确拟合训练数据：扩大模型&#x2F;减少正则化</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image28.png"
                      alt="image"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image29.png"
                      alt="image"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image30.png"
                      alt="image"
                ></p>
<h3 id="其他小技巧"><a href="#其他小技巧" class="headerlink" title="其他小技巧"></a>其他小技巧</h3><p>1.查看权重更新幅度与权重幅度的比值。如果太大，说明有错误。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image31.png"
                      alt="image"
                ></p>
<h1 id="3-After-training"><a href="#3-After-training" class="headerlink" title="3.After training"></a>3.After training</h1><h2 id="7-Model-Ensembles-模型集合"><a href="#7-Model-Ensembles-模型集合" class="headerlink" title="7.Model Ensembles 模型集合"></a>7.Model Ensembles 模型集合</h2><p>使用不同模型的集合，以他们结果的平均作为最终结果。通常最终会得到1-2%的改善。</p>
<p>Tips &amp; Tricks：1. 可以保存一个模型多个检查点，而不是训练多个模型。</p>
<ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>使学习率呈周期性变化，在学习率周期性最低点时，保存作为检查点。</li>
<li>Polyak averaging：（在生成模型中常用）使用运行时的模型权重的指数运行平均值，并在测试时使用该值。（方便消除一些迭代产生的噪声）</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="8-Transfer-Learning-迁移学习"><a href="#8-Transfer-Learning-迁移学习" class="headerlink" title="8.Transfer Learning 迁移学习"></a>8.Transfer Learning 迁移学习</h2><p>方法1： 在特定数据集训练后，去掉最后一层全连接层（FC layer），相当于得到一个特征向量。再根据特征向量建立新关系。</p>
<h3 id="细调（有时能带来较大提升）："><a href="#细调（有时能带来较大提升）：" class="headerlink" title="细调（有时能带来较大提升）："></a>细调（有时能带来较大提升）：</h3><ul>
<li>先完成预训练，提取特征向量再进行细调。（反向传播，更新权重）</li>
<li>降低学习率，一般调整至原训练的1&#x2F;10</li>
<li>有时可以将低层次保持不变，减少计算量</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image32.png"
                      alt="image"
                ></p>
<h3 id="总结（根据数据集类型，选择更合适的架构）："><a href="#总结（根据数据集类型，选择更合适的架构）：" class="headerlink" title="总结（根据数据集类型，选择更合适的架构）："></a>总结（根据数据集类型，选择更合适的架构）：</h3><p>如果<u>数据集很小</u>，Pretraining+Finetune 更有用。<u>收集更多数据比pretraining更高效</u>。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image33.png"
                      alt="image"
                ></p>
<h2 id="9-Distributed-Training-分布式训练"><a href="#9-Distributed-Training-分布式训练" class="headerlink" title="9.Distributed Training 分布式训练"></a>9.Distributed Training 分布式训练</h2><p>“数据并行”</p>
<p>在批处理维度上划分为n个图像，分给不同gpu独立运行，在末尾对梯度求和时合并数据。</p>
<h2 id="10-Large-Batch-Training-大批量训练"><a href="#10-Large-Batch-Training-大批量训练" class="headerlink" title="10.Large-Batch Training 大批量训练"></a>10.Large-Batch Training 大批量训练</h2><p>多块GPU同时训练时：按比例缩放批量、学习率。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image34.png"
                      alt="image"
                ></p>
<p><strong>”学习率热身“</strong>（学习率如果一开始过高，容易梯度爆炸）</p>
<p>在初始的0-5000次迭代，学习率<strong>从0开始线性增加</strong>。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture10%2611/image35.png"
                      alt="image"
                ></p>
]]></content>
      <categories>
        <category>CS231N</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2025/02/08/%E5%85%A8%E6%A0%88%E5%B7%A5%E7%A8%8B--Protobuf/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2025/02/08/%E5%85%A8%E6%A0%88%E5%B7%A5%E7%A8%8B--gRPC/</url>
    <content><![CDATA[<h4 id="Client-Server-model⭐"><a href="#Client-Server-model⭐" class="headerlink" title="Client-Server model⭐"></a>Client-Server model⭐</h4><p>Client-Server结构是一种经典的通信模型。它通常采取两层结构：</p>
<ul>
<li>服务器（Server）负责数据的处理。它有以下特征：<ul>
<li>等待来自客户端的请求</li>
<li>处理请求并传回结果</li>
</ul>
</li>
<li>客户端（Client）负责完成与用户的交互任务。它有以下特征：<ul>
<li>发送请求</li>
<li>等待直到收到响应</li>
</ul>
</li>
</ul>
<h4 id="IP-Address"><a href="#IP-Address" class="headerlink" title="IP Address"></a>IP Address</h4><p>IP Address(Internet Protocol address，网际协议地址)，是网际协议中用于标识发送或接受数据报的设<br>备的一串数字。</p>
<p>当设备连接网络后，设备将被分配一个IP地址，对于一个具体的设备而言，IP地址是独一无二的。IP地<br>址有两个主要的功能：<strong>标识主机</strong>（用户在互联网上可以识别）和<strong>网络寻址</strong>（允许计算机通过互联网发送<br>和接受数据）</p>
<p>常见的IP地址分为IPv4和IPv6两大类：</p>
<ul>
<li><p>IPv4：32位长，通常书写时以四组十进制数字组成，并以点分割，例如： 172.16.254.1 。</p>
</li>
<li><p>IPv6：128位长，通常书写时以八组十六进制数字组成，并以冒号分割，例如：</p>
<p>2001:db8:0：1234:0:567:8:1 。</p>
</li>
</ul>
<p>我们可以使用如下方法查询本机的IP地址：<br>windows： ipconfig<br>linux： ifconfig （可能需要使用 sudo apt-get install net-tools 进行安装）</p>
<blockquote>
<p><u>一个特殊的IP地址： 127.0.0.1</u><br>尽管现在有大量可用的 IP 地址，但为了防止编程冲突的特定目的，刻意保留一些地址，甚至是地址范围是很方便的。<br>127.0.0.1 就是其中一个。它表示的是<strong>主机环回地址</strong>，表示的是任何数据包都不应该离开计算机，计算机本身即为接收者。</p>
<p>当我们需要在本地测试一些网站服务，或者只想在本地设备上运行只有本地设备可以访问的服务，就可以使用 127.0.0.1 。</p>
</blockquote>
<h4 id="Port"><a href="#Port" class="headerlink" title="Port"></a>Port</h4>]]></content>
  </entry>
  <entry>
    <title>CS231N Lecture12 RNN</title>
    <url>/2025/02/24/CS231N/CS231N-Lecture12/</url>
    <content><![CDATA[<h1 id="Lecture12-RNN"><a href="#Lecture12-RNN" class="headerlink" title="Lecture12 RNN"></a>Lecture12 RNN</h1><p>循环神经网络：以某种顺序的过程，处理顺序问题和非顺序的问题</p>
<h2 id="隐藏层的更新"><a href="#隐藏层的更新" class="headerlink" title="隐藏层的更新"></a>隐藏层的更新</h2><p>如图，呈递归关系。</p>
<p>注意：在每个时间步中，使用的f函数以及参数都是相同的。即一直使用相同的权重矩阵。其中x_{t}为当前时间步的输入向量。</p>
<p><a href="#">h_{t}&#x3D;f_{W}(h_{t-1},x_{t})</a></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image1.png"
                      alt="image"
                ></p>
<h2 id="举例：Vanilla-RNN"><a href="#举例：Vanilla-RNN" class="headerlink" title="举例：Vanilla RNN"></a>举例：Vanilla RNN</h2><p>h_{t-1}乘以矩阵，x_{t}乘以另一个矩阵，然后使用tanh得到新状态，输出y_{t}是h_{t}的线性变换</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image2.png"
                      alt="image"
                ></p>
<h2 id="用计算图来理解RNN："><a href="#用计算图来理解RNN：" class="headerlink" title="用计算图来理解RNN："></a>用计算图来理解RNN：</h2><h3 id="1-多对多"><a href="#1-多对多" class="headerlink" title="1.多对多"></a>1.多对多</h3><p>y-&gt;L 求loss（举例：可以应用交叉熵损失 ）再对每个时间步的loss求和得到最终的LOSS</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image3.png"
                      alt="image"
                ></p>
<h3 id="2-多对一"><a href="#2-多对一" class="headerlink" title="2.多对一"></a>2.多对一</h3><p>例如：视频分类</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image4.png"
                      alt="image"
                ></p>
<h3 id="3-一对多"><a href="#3-一对多" class="headerlink" title="3.一对多"></a>3.一对多</h3><p>例如：分析图像生成字幕…</p>
<p>输入0或者将前一步输出再输入</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image5.png"
                      alt="image"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image6.png"
                      alt="image"
                ></p>
<h2 id="Sequence-2-Sequence：多对一-一对多"><a href="#Sequence-2-Sequence：多对一-一对多" class="headerlink" title="Sequence 2 Sequence：多对一+一对多"></a>Sequence 2 Sequence：多对一+一对多</h2><p>类似于编码器＋译码器；</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image7.png"
                      alt="image"
                ></p>
<h3 id="举例：character-level-language-model"><a href="#举例：character-level-language-model" class="headerlink" title="举例：character-level language model"></a>举例：character-level language model</h3><p>使用one-hot编码.</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image8.png"
                      alt="image"
                ></p>
<h3 id="举例：取样的character-level-language-model"><a href="#举例：取样的character-level-language-model" class="headerlink" title="举例：取样的character-level language model"></a>举例：取样的character-level language model</h3><p>在测试时，每采样一个字符，反馈给模型，继续进行生成。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image9.png"
                      alt="image"
                ></p>
<h3 id="单热向量-one-hot"><a href="#单热向量-one-hot" class="headerlink" title="单热向量 one-hot"></a>单热向量 one-hot</h3><p>单热向量的矩阵乘法只是从权重矩阵中提取一列，没有什么意义。</p>
<p>所以，我们经常在输入层和隐藏层之间放置一个单独的嵌入层，用来进行之后的计算。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image10.png"
                      alt="image"
                ></p>
<h2 id="时间截断反向传播：Truncated-Backpropagation-through-time"><a href="#时间截断反向传播：Truncated-Backpropagation-through-time" class="headerlink" title="时间截断反向传播：Truncated Backpropagation through time"></a>时间截断反向传播：Truncated Backpropagation through time</h2><p>取一个初始子集（10-100token），将其前序传递展开，求loss，然后通过初始块反向传播，对权重矩阵进行更新。</p>
<p>永远向前携带隐藏层，但只能反向传播一些较小的步骤</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image11.png"
                      alt="image"
                ></p>
<h2 id="使用举例："><a href="#使用举例：" class="headerlink" title="使用举例："></a>使用举例：</h2><h3 id="Image-Captioning"><a href="#Image-Captioning" class="headerlink" title="Image Captioning"></a>Image Captioning</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image12.png"
                      alt="image"
                ></p>
<p>迁移学习。首先使用一个在imagenet预训练过的CNN模型，去掉最后两层。</p>
<p>（x：输入，h：隐藏层，v：cnn输入的特征向量，每个对应不同的矩阵）</p>
<p>使用[start]和[end]的token作为一次分析的开始指令与结束指令。</p>
<h3 id="Visual-Q-uestion-Answering（VQA）"><a href="#Visual-Q-uestion-Answering（VQA）" class="headerlink" title="Visual Q uestion Answering（VQA）"></a>Visual Q uestion Answering（VQA）</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image13.png"
                      alt="image"
                ></p>
<h3 id="Visual-Dialog-Conversations-about-images"><a href="#Visual-Dialog-Conversations-about-images" class="headerlink" title="Visual Dialog: Conversations about images"></a>Visual Dialog: Conversations about images</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image14.png"
                      alt="image"
                ></p>
<h3 id="Visual-Language-Navigation-Go-to-the-living-room"><a href="#Visual-Language-Navigation-Go-to-the-living-room" class="headerlink" title="Visual Language Navigation: Go to the living room"></a>Visual Language Navigation: Go to the living room</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image15.png"
                      alt="image"
                ></p>
<h2 id="梯度流"><a href="#梯度流" class="headerlink" title="梯度流"></a>梯度流</h2><p>以Vanilla RNN 举例。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image16.png"
                      alt="image"
                ></p>
<p>如果梯度&gt;1，最终会爆炸型增长；&lt;1，最终无限趋于零。</p>
<h3 id="1-Gradient-clipping-梯度裁剪-：克服梯度爆炸"><a href="#1-Gradient-clipping-梯度裁剪-：克服梯度爆炸" class="headerlink" title="1.Gradient clipping 梯度裁剪 ：克服梯度爆炸"></a>1.Gradient clipping 梯度裁剪 ：克服梯度爆炸</h3><p>在反向传播时，检查梯度的范数，如果过大，将其裁剪，再继续反向传播</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image17.png"
                      alt="image"
                ></p>
<h3 id="2-Long-Short-Term-Memory-LSTM-：克服梯度消失"><a href="#2-Long-Short-Term-Memory-LSTM-：克服梯度消失" class="headerlink" title="2.Long Short Term Memory (LSTM)：克服梯度消失"></a>2.Long Short Term Memory (LSTM)：克服梯度消失</h3><p>在每个时间步使用两个向量：<br>$$<br>Cellstate(ct)+Hiddenstate(ht)<br>$$<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image18.png"
                      alt="image"
                ></p>
<h3 id="如何作用的？"><a href="#如何作用的？" class="headerlink" title="如何作用的？"></a>如何作用的？</h3><p>使用四个gate（i&#x2F;f&#x2F;o&#x2F;g）来决定下一个LSTM状态。</p>
<p>公式如图。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image19.png"
                      alt="image"
                ></p>
<p>对于LSTM，其四个门<strong>来源于h_{t-1}与x_{t}与权重矩阵W相乘，然后输出四个门</strong>。</p>
<p>而反向梯度传播，<strong>只与f gate 进行乘法</strong>（如果f接近0，有可能破坏信息，接近1不可能破坏）， 解决了梯度消失的问题。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image20.png"
                      alt="image"
                ></p>
<h2 id="多层-深度循环"><a href="#多层-深度循环" class="headerlink" title="多层&#x2F;深度循环"></a>多层&#x2F;深度循环</h2><p>将隐藏层作为序列输入至另一个RNN….</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image21.png"
                      alt="image"
                ></p>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image22.png"
                      alt="image"
                ></p>
<h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N/CS231N-Lecture12/image23.png"
                      alt="image"
                ></p>
]]></content>
      <categories>
        <category>CS231N</category>
      </categories>
      <tags>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
</search>
