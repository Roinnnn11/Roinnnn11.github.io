<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2024/11/10/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a class="link"   href="https://hexo.io/" >Hexo <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>! This is your very first post. Check <a class="link"   href="https://hexo.io/docs/" >documentation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> for more info. If you get any problems when using Hexo, you can find the answer in <a class="link"   href="https://hexo.io/docs/troubleshooting.html" >troubleshooting <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start-你好"><a href="#Quick-Start-你好" class="headerlink" title="Quick Start 你好"></a>Quick Start 你好</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/writing.html" >Writing <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/server.html" >Server <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/generating.html" >Generating <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><div class="highlight-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></div>

<p>More info: <a class="link"   href="https://hexo.io/docs/one-command-deployment.html" >Deployment <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
]]></content>
  </entry>
  <entry>
    <title>PyTorch101 --PyTorch基本框架</title>
    <url>/2024/11/12/PyTorch101/</url>
    <content><![CDATA[<h3 id="一、Tensor-张量"><a href="#一、Tensor-张量" class="headerlink" title="一、Tensor 张量"></a>一、Tensor 张量</h3><p>定义：The number of dimensions is the <strong>rank</strong> of the tensor；例：tensor([1,2,3])的秩为1</p>
<p>​		the <strong>shape</strong> of a tensor is a tuple of integers giving the size of the array along each dimension 例：tensor([1,2,3])的shape为[3]</p>
<h4 id="1-构建-访问tensor"><a href="#1-构建-访问tensor" class="headerlink" title="1.构建&amp;访问tensor"></a>1.构建&amp;访问tensor</h4><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])<span class="comment">#构建tensor张量</span></span><br><span class="line">x.dim()<span class="comment">#返回纬度值</span></span><br><span class="line">x.shape<span class="comment">#返回：torch.Size([3,3]),可以当作turple使用</span></span><br><span class="line">x[<span class="number">0</span>][<span class="number">0</span>]=<span class="number">1</span><span class="comment">#赋值操作</span></span><br><span class="line">x[<span class="number">0</span>][<span class="number">0</span>].item()<span class="comment">#将PyTorch的标量转换为python标量</span></span><br></pre></td></tr></table></figure></div>



<h4 id="2-帮助构建tensor的函数"><a href="#2-帮助构建tensor的函数" class="headerlink" title="2.帮助构建tensor的函数"></a>2.帮助构建tensor的函数</h4><p>常用的函数有：</p>
<ul>
<li><pre><code class="python">- x=torch.zeros（*size） #全为0的tensor
- x=torch.ones(*size)#全为1的tensor
- x=torch.rand(*size)#为0-1随机数
- x=torch.full（*size*， *fill_value*， ***， *out* *=* *None*， *dtype* *=* None， *layout* *=* *torch.strided*， *device* *=* *None*， *requires_grad* *=* *False*）#→ Tensor
- x=torch.from_numpy( ndarray ) #→ Tensor
- x=torch.arange( *start=0* , *end* , *step=1* , * , out=None , *dtype=None* , layout *=torch.strided* , *device=None* , *require_grad=False* )#→Tensor &lt;u&gt;#左闭右开区间&lt;/u&gt;（torch.range &lt;u&gt;#左闭右闭&lt;/u&gt;）
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">You can find a **full list of tensor creation operations** [in the documentation](https://www.google.com/url?q=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Ftorch.html%23creation-ops).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 3.tensor的数据形式</span><br><span class="line"></span><br><span class="line">tensor会根据数字自动设定形式，你也可以为其赋予特定格式如：int16、uint8、float32等等。</span><br><span class="line"></span><br><span class="line">可以使用  .to( )来修改dtype，如：x0.to(torch.float32)</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">x1=torch.zeros_like(x0) #构建一个size和dtype与x0一样的</span><br><span class="line">x2=x0.new_zeros(4,5)#size自定，dtype和x0一样</span><br><span class="line">x3=torch.ones(6,7).to(x0)#和上一行相同</span><br></pre></td></tr></table></figure></div>
</code></pre>
</li>
</ul>
<p>常用的数据形式：</p>
<ul>
<li><code>torch.float32</code>: Standard floating-point type; used to store learnable parameters, network activations, etc. Nearly all arithmetic is done using this type.</li>
<li><code>torch.int64</code>: Typically used to store indices</li>
<li><code>torch.bool</code>: Stores boolean values: 0 is false and 1 is true</li>
<li><code>torch.float16</code>: Used for mixed-precision arithmetic, usually on NVIDIA GPUs with <a class="link"   href="https://www.google.com/url?q=https://www.nvidia.com/en-us/data-center/tensorcore/" >tensor cores <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>. You won’t need to worry about this datatype in this course.</li>
</ul>
<h4 id="4-Tensor-Indexing"><a href="#4-Tensor-Indexing" class="headerlink" title="4.Tensor Indexing"></a>4.Tensor Indexing</h4><p>tensor也能进行“切片”，多维度也行</p>
]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2024/11/19/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    <content><![CDATA[<hr>
<h2 id="title-CS231N-Lecture1-Linear-Classifiers–线性分类器"><a href="#title-CS231N-Lecture1-Linear-Classifiers–线性分类器" class="headerlink" title="title:CS231N Lecture1:Linear Classifiers–线性分类器"></a>title:CS231N Lecture1:Linear Classifiers–线性分类器</h2><h1 id="Linear-Classifiers–线性分类器"><a href="#Linear-Classifiers–线性分类器" class="headerlink" title="Linear Classifiers–线性分类器"></a>Linear Classifiers–线性分类器</h1><h3 id="一、如何理解线性分类器"><a href="#一、如何理解线性分类器" class="headerlink" title="一、如何理解线性分类器"></a>一、如何理解线性分类器</h3><h5 id="一、代数观点分析"><a href="#一、代数观点分析" class="headerlink" title="一、代数观点分析"></a>一、代数观点分析</h5><p>线性分类器：权重矩阵W和像素X之间的矩阵相乘，再加上b</p>
<p><em>如果输入数据具有native vector form，可以将b合并至W矩阵中处理**（针对线性分类是好的方法，对于卷积并非）</em></p>
<p><u>feature：预测也是线性的，放大&#x2F;缩小所有像素，会让所有预测值都放大&#x2F;缩小</u></p>
<p>（如下图）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/image-20241119183936618.png"
                      alt="image"
                ></p>
<h5 id="二、视觉角度分析"><a href="#二、视觉角度分析" class="headerlink" title="二、视觉角度分析"></a>二、视觉角度分析</h5><p>另一种处理方法：<u>将权重矩阵reshape到跟输入图像一样，实现<strong>“template matching”</strong></u></p>
<p>例：取每一行数字，组成2x2 shape（图像假设为2x2），同时b也分类。</p>
<p><strong>“template matching”</strong>(模式匹配)</p>
<p>按上述处理之后，每一行都是一个种类，当他们匹配上时，值会最大。有一种视觉上的模式匹配（如下图）</p>
<p>（visual viewpoint）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/image-20241119185407644.png"
                      alt="image"
                ></p>
<p><u>（缺点：对input图像的依赖严重，对其content（环境）依赖，旋转图片后无法识别）</u></p>
<h5 id="三、几何观点"><a href="#三、几何观点" class="headerlink" title="三、几何观点"></a>三、几何观点</h5><p>可以认为，像素图像是一个高维欧几里得空间，每个种类都有对应的一个超平面，将空间切割。（如下图）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/image-20241119191101704.png"
                      alt="image"
                ></p>
<h3 id="二、如何选择矩阵W-——使用损失函数来量化评估W"><a href="#二、如何选择矩阵W-——使用损失函数来量化评估W" class="headerlink" title="二、如何选择矩阵W ——使用损失函数来量化评估W"></a>二、如何选择矩阵W ——使用损失函数来量化评估W</h3><h5 id="1️⃣loss：multi-class-SVM损失（图像分类）"><a href="#1️⃣loss：multi-class-SVM损失（图像分类）" class="headerlink" title="1️⃣loss：multi-class SVM损失（图像分类）"></a>1️⃣loss：multi-class SVM损失（图像分类）</h5><p>特征：如果一个种类被正确区分，那么改变预测分数，不再影响损失。（达到零损失）</p>
<p>（如图，图中为铰链损失）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/IMG_0514.PNG"
                      alt="\images\IMG_0514.PNG"
                ></p>
<h5 id="2️⃣Cross-Entropy-Loss（multinomial-logistic-regression）"><a href="#2️⃣Cross-Entropy-Loss（multinomial-logistic-regression）" class="headerlink" title="2️⃣Cross- Entropy Loss（multinomial logistic regression）"></a>2️⃣Cross- Entropy Loss（multinomial logistic regression）</h5><p>want to interpret raw classifier scores as probabilities.（使用概率来评分）</p>
<p>使用softmax function 求概率 （重要工具）</p>
<p><strong>对loss的计算：L &#x3D; -logP（Y&#x3D;y_{i}&#x2F;X&#x3D;x_{i}）</strong></p>
<p>feature: 这个loss函数永远不会达到0️⃣损失</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/IMG_0522.PNG"
                      alt="\images\IMG_0522.PNG"
                ></p>
<p>对预测值和理论值进行评估<strong>（Kullback-Leibler divergence）</strong></p>
<p><strong>cross- Entropy：H（P,Q）&#x3D;H(p)+D_{KL}（P||Q）</strong></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/IMG_0523.PNG"
                      alt="\images\IMG_0523.PNG"
                ></p>
<h3 id="三、对矩阵进行优化"><a href="#三、对矩阵进行优化" class="headerlink" title="三、对矩阵进行优化"></a>三、对矩阵进行优化</h3><p>Regularization（正则化）—— prevent the model from doing too well</p>
<p>lamda —— 控制正则化的超参</p>
<p>公式如图</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/IMG_0516.PNG"
                      alt="\images\IMG_0516.PNG"
                ></p>
<p>正则化简单实例：L2、L1、Elastic net（L1+L2）（如上图左下）</p>
<p>正则化目的：1️⃣表达偏好</p>
<p>​						2️⃣避免过拟合prefer simple models that generalize well</p>
<p>​						3️⃣添加曲率改善优化adding curvature improve optimization</p>
<p>举例说明正则化的作用：</p>
<p>1️⃣比如可以通过调整正则化、考虑全局或者专注于一个参数</p>
<p>如果有噪声或者很多特征,也适用。<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture1/IMG_0517.PNG"
                      alt="images\IMG_0517.PNG"
                ></p>
<p>2️⃣</p>
<p>避免过拟合。如图，曲线是添加正则化后，合理地减少了噪声干扰</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/IMG_0518.PNG"
                      alt="image"
                ></p>
]]></content>
  </entry>
  <entry>
    <title></title>
    <url>/2024/12/07/%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<hr>
<h2 id="title-CS231N-Lecture2-Optimization–优化"><a href="#title-CS231N-Lecture2-Optimization–优化" class="headerlink" title="title:CS231N Lecture2:Optimization–优化"></a>title:CS231N Lecture2:Optimization–优化</h2><p>2️⃣ follow the slope</p>
<p>直接计算导数、使用导数（deltaloss&#x2F;delta-h）</p>
<p>缺点：对求导计算缓慢，如果矩阵&#x2F;维度过大</p>
<p>&#x3D;&#x3D;&#x3D;用一个方程表示梯度（反向传播 第六节中讲述）</p>
<p><strong>Computing Gradients(计算梯度)</strong></p>
<p>numeric&#x2F;analytic （计算出值&#x2F;分析）</p>
<p>一般使用数字梯度来检验对解析梯度的推导</p>
<p>（pytorch提供了一个函数gradcheck、实现相似的功能）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0531.PNG"
                      alt="image"
                ></p>
<h3 id="gradient-descent梯度下降"><a href="#gradient-descent梯度下降" class="headerlink" title="gradient descent梯度下降"></a><u>gradient descent梯度下降</u></h3><p>包含的超参：</p>
<p><u>1️⃣初始化权重的方法</u></p>
<p><u>2️⃣循环次数：受计算预算和时间限制</u></p>
<p><u>3️⃣学习率learning rate：</u></p>
<p>较大：收敛较快</p>
<p>较小：不容易数字爆炸、但时间较长</p>
<p>梯度下降的feature：使用全部数据，当数据集较大时，不适用</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0532.PNG"
                      alt="image"
                ></p>
<h3 id="batch-gradient-descent（批量梯度下降）"><a href="#batch-gradient-descent（批量梯度下降）" class="headerlink" title="batch gradient descent（批量梯度下降）"></a><u>batch gradient descent（批量梯度下降）</u></h3><p>选择批量，进行求loss和计算梯度</p>
<p>SGD随机梯度下降</p>
<h3 id="stochastic-gradient-descentSGD随机梯度下降"><a href="#stochastic-gradient-descentSGD随机梯度下降" class="headerlink" title="stochastic gradient descentSGD随机梯度下降"></a><u>stochastic gradient descentSGD随机梯度下降</u></h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0533.PNG"
                      alt="image"
                ></p>
<p>存在的问题：</p>
<p><u>局部最小、鞍点&#x2F;收敛速度一方过大一方过小</u></p>
<p><strong>更智能的方法：</strong></p>
<h4 id="1-sgd-动量更新"><a href="#1-sgd-动量更新" class="headerlink" title="1.sgd+动量更新"></a>1.sgd+动量更新</h4><p>初始化相同、用梯度算出”dw”的值，用来更新速度向量，（想象球滚落）（类似于对梯度计算历史平均值）</p>
<p>超参数：<strong>rho– decay rate</strong></p>
<p><u>（当抵达局部最小&#x2F;鞍点，仍有残余速度去跳出局部，更平滑地处理高纬度问题）</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0555.PNG"
                      alt="image"
                ></p>
<h4 id="2-Nesterov-动量更新"><a href="#2-Nesterov-动量更新" class="headerlink" title="2.Nesterov 动量更新"></a>2.Nesterov 动量更新</h4><p>如图</p>
<p>“Look ahead”会对下一时刻的梯度进行预测</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0559.PNG"
                      alt="image"
                ></p>
<p>两者对比：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0557.PNG"
                      alt="image"
                ></p>
<p><u>sgd特征：1.可能会overshoot（到达底端后，会有反向动量存在，往回走一段距离，如下图）</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0560.PNG"
                      alt="image"
                ></p>
<h3 id="Adaptive-learning-rates自适应学习率"><a href="#Adaptive-learning-rates自适应学习率" class="headerlink" title="Adaptive learning rates自适应学习率"></a>Adaptive learning rates自适应学习率</h3><h4 id="1-AdaGrad算法"><a href="#1-AdaGrad算法" class="headerlink" title="1.AdaGrad算法"></a>1.AdaGrad算法</h4><p>（通过<strong>除以梯度</strong>，抑制&#x2F;提高运动速度）</p>
<p><u>问题：grad—squared不断增加（趋于无限大），学习率不断下降，可能在达到结果前停止</u></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0561.PNG"
                      alt="image"
                ></p>
<h4 id="2-RMSprop（对AdaGrad的改善）"><a href="#2-RMSprop（对AdaGrad的改善）" class="headerlink" title="2.RMSprop（对AdaGrad的改善）"></a>2.RMSprop（对AdaGrad的改善）</h4><p>添加参数decay_rate 来降低grad_squared,  类似加入摩擦系数</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0562.PNG"
                      alt="image"
                ></p>
<h3 id="Adam（RMSProp-动量）"><a href="#Adam（RMSProp-动量）" class="headerlink" title="Adam（RMSProp+动量）"></a>Adam（RMSProp+动量）</h3><p>——非常强大的优化算法，适用于不同的任务，超参的调整范围也较小</p>
<p>将两种算法结合，存在问题：如果beta2趋于1，那么优化第一步可能走出很大一步（除以近似0的数）</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0563.PNG"
                      alt="image"
                ></p>
<p>对其进行改善：添加<strong>Bias Corection（偏差校正）</strong>，让其更稳健。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0564.PNG"
                      alt="image"
                ></p>
<h3 id="二阶优化方法：L-BFGS（准确但不实际）"><a href="#二阶优化方法：L-BFGS（准确但不实际）" class="headerlink" title="二阶优化方法：L-BFGS（准确但不实际）"></a>二阶优化方法：L-BFGS（准确但不实际）</h3><p>使用Hessian矩阵，可以使对学习率&#x2F;步长的选择更加合理。但数据量巨大，只适合低维优化问题</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0567.PNG"
                      alt="image"
                ></p>
<h3 id="总结（对优化算法的比较）"><a href="#总结（对优化算法的比较）" class="headerlink" title="总结（对优化算法的比较）"></a>总结（对优化算法的比较）</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://github.com/Roinnnn11/Markdown_PNG/raw/main/CS231N-Lecture2/IMG_0566.PNG"
                      alt="image"
                ></p>
]]></content>
  </entry>
</search>
